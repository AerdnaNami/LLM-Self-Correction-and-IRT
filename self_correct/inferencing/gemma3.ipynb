{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "gsm_data = pd.read_csv('../data/preprocessed/gsm_data.csv')\n",
    "gsm_data = gsm_data.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DONT RUN THE ABOVE AFTER RUNNING ONCE if running the below code more than 1 time** \\\n",
    "It ensures all types of questions have a fair chance of being answered by the LLM \\\n",
    "GSM-symbolic groups questions together based on template. This causes bias for the first few templates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def ask_gemma3(prompt):\n",
    "  client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=\"...\",\n",
    "  )\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "    model=\"google/gemma-3-27b-it\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "  if completion.choices[0].message.content != None:\n",
    "    return completion.choices[0].message.content\n",
    "  else:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_final_ans(response):\n",
    "    answer = response.replace('*', '')\n",
    "    answer = answer.replace(\",\" ,'')\n",
    "    answer = answer.lower().split('answer: ')\n",
    "\n",
    "    try:\n",
    "        final = re.findall(r\"\\d+\", answer[-1])[0]\n",
    "    except IndexError:\n",
    "        final = None\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect 50 incorrect solutions first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def collect_responses(df, mistakes):\n",
    "    llm_response = []\n",
    "    n = 0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if n==mistakes:\n",
    "            return llm_response\n",
    "        \n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Answer the following question. Please use Chain of Thought and number each step:\n",
    "        {question}\n",
    "        \n",
    "        Have your final answer strictly at the end of your response in this form:\n",
    "        Answer: \n",
    "        \"\"\"\n",
    "        \n",
    "        final = None\n",
    "    \n",
    "        while final == None:\n",
    "            response = ask_gemma3(prompt)\n",
    "            final = extract_final_ans(response)\n",
    "        \n",
    "        ref_final = answer.splitlines()\n",
    "        ref_final = ref_final[-1].replace(\"#\",'')\n",
    "        ref_final = ref_final.strip()\n",
    "\n",
    "        if ref_final != final:\n",
    "            n += 1\n",
    "            llm_response.append({'question': question, 'ref_answer': answer, 'llm_answer': response, 'final_ans': final})\n",
    "            temp = pd.DataFrame(llm_response)\n",
    "            temp.to_csv('../data/responses/temp.csv')\n",
    "            \n",
    "        print(\"final_ans: \", final)\n",
    "        print(\"ref_ans: \", ref_final)\n",
    "        print('iteration: ', n)\n",
    "\n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_gsm = collect_responses(gsm_data, 100) # gather more than 50 incorrct solutions to be able to ensure you have 50 valid samples after filtering\n",
    "llm_gsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"../data/responses\"\n",
    "df_gsm= pd.DataFrame(llm_gsm)\n",
    "df_gsm.to_csv(save_directory + '/gemma3_response.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self correction process - w/ error location and error type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_responses = pd.read_csv('../data/responses/gemma3/gemma3-27b_response_annotated.csv')\n",
    "df_responses = df_responses.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def self_correct(df):\n",
    "    llm_response_corrected = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        solution = row['llm_answer']\n",
    "        error_step = row['Error Step']\n",
    "        error_type = re.sub(r'^[\\d.]+', '', row['Error Reason'].strip('()')).lstrip()\n",
    "        answer = row['ref_answer']\n",
    "\n",
    "        correction_prompt = f\"\"\"\n",
    "        Here are definitions of error types:\n",
    "        - Incorrect value of variable cited or used: If the numerical value of a known (input or intermediate) variable is cited or used incorrectly.\n",
    "        - Value of a known variable calculated again: If the value of an input or intermediate (determined mid solution) variable is calculated when it's value is already provided/ known.\n",
    "        - Irrelevant/incorrect variable cited or used: If a variable in a solution is found to lie outside of the ideal variable-set of the solutioin (thereby making the solution innacurate).\n",
    "        - Relevant variable missing: If any of the required variables in a solutioin for a target variable is missing (thereby making the solution innacurate).\n",
    "        - Unit Inconsistency: If during the execution of an expression, two or more unit incompatible quantities are operated between.\n",
    "        - Calculation Error: If the resultant numerical value of an operation or set of operations between numerical values is incorrect. \n",
    "        - Incorrect Value: If the numerical value of the final answer is incorrect.\n",
    "        \n",
    "        The following solution of the given question is generated using the same model as you. Consider the question and solution that uses the Chain of Thought strategy with each step numbered.\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Solution: {solution}\n",
    "\n",
    "        You made a mistake in Step {int(error_step)} due to {error_type}.  Can you revise your solution and carefully solve it again?\n",
    "        \"\"\"\n",
    "\n",
    "        response = ask_gemma3(correction_prompt)\n",
    "        final = extract_final_ans(response)\n",
    "\n",
    "        ref_final = answer.splitlines()\n",
    "        ref_final = ref_final[-1].replace(\"#\",'')\n",
    "        ref_final = ref_final.strip()\n",
    "\n",
    "        print(\"Ref answer: \", ref_final)\n",
    "        print(\"LLM initial answer: \", row['final_ans'])\n",
    "        print(\"LLM self revised: \", final)\n",
    "        \n",
    "        llm_response_corrected.append({'question': question, 'ref_answer': answer, 'llm_initial_answer': row['llm_answer'],\n",
    "                                        'initial_final_ans': row['final_ans'], 'llm_revised_answer': response,\n",
    "                                        'revised_final_ans': final, 'error_type': row['Error Reason'], \n",
    "                                        'error_step': row['Error Step'], 'total_steps': row['Total Steps'],\n",
    "                                        'correct': ref_final==final})\n",
    "    \n",
    "    return llm_response_corrected\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref answer:  5\n",
      "LLM initial answer:  25\n",
      "LLM self revised:  25\n",
      "Ref answer:  1\n",
      "LLM initial answer:  8\n",
      "LLM self revised:  1\n",
      "Ref answer:  4230\n",
      "LLM initial answer:  29610\n",
      "LLM self revised:  4230\n",
      "Ref answer:  1225\n",
      "LLM initial answer:  7350\n",
      "LLM self revised:  1225\n",
      "Ref answer:  63\n",
      "LLM initial answer:  75\n",
      "LLM self revised:  75\n",
      "Ref answer:  90\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  100\n",
      "Ref answer:  40\n",
      "LLM initial answer:  35\n",
      "LLM self revised:  40\n",
      "Ref answer:  544\n",
      "LLM initial answer:  613\n",
      "LLM self revised:  544\n",
      "Ref answer:  33\n",
      "LLM initial answer:  43\n",
      "LLM self revised:  9\n",
      "Ref answer:  3\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  770\n",
      "LLM initial answer:  2310\n",
      "LLM self revised:  2310\n",
      "Ref answer:  10\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  1610\n",
      "LLM initial answer:  14\n",
      "LLM self revised:  14\n",
      "Ref answer:  343\n",
      "LLM initial answer:  377\n",
      "LLM self revised:  528\n",
      "Ref answer:  42\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  42\n",
      "Ref answer:  64\n",
      "LLM initial answer:  24\n",
      "LLM self revised:  64\n",
      "Ref answer:  39\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  24\n",
      "LLM initial answer:  48\n",
      "LLM self revised:  24\n",
      "Ref answer:  8\n",
      "LLM initial answer:  48\n",
      "LLM self revised:  0\n",
      "Ref answer:  11\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  74\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  74\n",
      "Ref answer:  3\n",
      "LLM initial answer:  11\n",
      "LLM self revised:  11\n",
      "Ref answer:  36\n",
      "LLM initial answer:  24\n",
      "LLM self revised:  56\n",
      "Ref answer:  1\n",
      "LLM initial answer:  6\n",
      "LLM self revised:  12\n",
      "Ref answer:  10\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  10\n",
      "Ref answer:  596\n",
      "LLM initial answer:  536\n",
      "LLM self revised:  231\n",
      "Ref answer:  72\n",
      "LLM initial answer:  90\n",
      "LLM self revised:  66\n",
      "Ref answer:  2250\n",
      "LLM initial answer:  11250\n",
      "LLM self revised:  2250\n",
      "Ref answer:  464\n",
      "LLM initial answer:  492\n",
      "LLM self revised:  492\n",
      "Ref answer:  396\n",
      "LLM initial answer:  434\n",
      "LLM self revised:  373\n",
      "Ref answer:  1\n",
      "LLM initial answer:  8\n",
      "LLM self revised:  8\n",
      "Ref answer:  371\n",
      "LLM initial answer:  423\n",
      "LLM self revised:  360\n",
      "Ref answer:  59\n",
      "LLM initial answer:  35\n",
      "LLM self revised:  59\n",
      "Ref answer:  654\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  39\n",
      "Ref answer:  640\n",
      "LLM initial answer:  726\n",
      "LLM self revised:  648\n",
      "Ref answer:  8\n",
      "LLM initial answer:  168\n",
      "LLM self revised:  168\n",
      "Ref answer:  20\n",
      "LLM initial answer:  5\n",
      "LLM self revised:  30\n",
      "Ref answer:  596\n",
      "LLM initial answer:  536\n",
      "LLM self revised:  536\n",
      "Ref answer:  72\n",
      "LLM initial answer:  90\n",
      "LLM self revised:  1\n",
      "Ref answer:  2250\n",
      "LLM initial answer:  11250\n",
      "LLM self revised:  2250\n",
      "Ref answer:  10\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  100\n",
      "Ref answer:  7\n",
      "LLM initial answer:  9\n",
      "LLM self revised:  7\n",
      "Ref answer:  2538\n",
      "LLM initial answer:  10152\n",
      "LLM self revised:  10152\n",
      "Ref answer:  99\n",
      "LLM initial answer:  93\n",
      "LLM self revised:  33\n",
      "Ref answer:  14\n",
      "LLM initial answer:  84\n",
      "LLM self revised:  0\n",
      "Ref answer:  32\n",
      "LLM initial answer:  94\n",
      "LLM self revised:  86\n",
      "Ref answer:  220\n",
      "LLM initial answer:  308\n",
      "LLM self revised:  220\n",
      "Ref answer:  26\n",
      "LLM initial answer:  52\n",
      "LLM self revised:  26\n",
      "Ref answer:  78\n",
      "LLM initial answer:  71\n",
      "LLM self revised:  78\n"
     ]
    }
   ],
   "source": [
    "llm_self_corrected = self_correct(df_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"../data/responses\"\n",
    "\n",
    "df_corrected = pd.DataFrame(llm_self_corrected)\n",
    "df_corrected.to_csv(save_directory + '/gemma3/gemma3-27b_selfcorrected_errorloc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self correction process - general error location w/ error type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def self_correct(df):\n",
    "    llm_response_corrected = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        solution = row['llm_answer']\n",
    "        error_type = re.sub(r'^[\\d.]+', '', row['Error Reason'].strip('()')).lstrip()\n",
    "        answer = row['ref_answer']\n",
    "\n",
    "\n",
    "        if row['Error Step'] <= (row['Total Steps']/2):\n",
    "            prompt = f\"\"\"\n",
    "            Here are definitions of error types:\n",
    "            - Incorrect value of variable cited or used: If the numerical value of a known (input or intermediate) variable is cited or used incorrectly.\n",
    "            - Value of a known variable calculated again: If the value of an input or intermediate (determined mid solution) variable is calculated when it's value is already provided/ known.\n",
    "            - Irrelevant/incorrect variable cited or used: If a variable in a solution is found to lie outside of the ideal variable-set of the solutioin (thereby making the solution innacurate).\n",
    "            - Relevant variable missing: If any of the required variables in a solutioin for a target variable is missing (thereby making the solution innacurate).\n",
    "            - Unit Inconsistency: If during the execution of an expression, two or more unit incompatible quantities are operated between.\n",
    "            - Calculation Error: If the resultant numerical value of an operation or set of operations between numerical values is incorrect. \n",
    "            - Incorrect Value: If the numerical value of the final answer is incorrect.\n",
    "\n",
    "            The following solution is generated using the same model as you. Consider the following question and solution that uses the Chain of Thought strategy with each step numbered.\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Solution: {solution}\n",
    "\n",
    "            You made a mistake in the upper half of your solution due to {error_type}.  Can you revise your solution and carefully solve it again?\n",
    "            \"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "            Here are definitions of error types:\n",
    "            - Incorrect value of variable cited or used: If the numerical value of a known (input or intermediate) variable is cited or used incorrectly.\n",
    "            - Value of a known variable calculated again: If the value of an input or intermediate (determined mid solution) variable is calculated when it's value is already provided/ known.\n",
    "            - Irrelevant/incorrect variable cited or used: If a variable in a solution is found to lie outside of the ideal variable-set of the solutioin (thereby making the solution innacurate).\n",
    "            - Relevant variable missing: If any of the required variables in a solutioin for a target variable is missing (thereby making the solution innacurate).\n",
    "            - Unit Inconsistency: If during the execution of an expression, two or more unit incompatible quantities are operated between.\n",
    "            - Calculation Error: If the resultant numerical value of an operation or set of operations between numerical values is incorrect. \n",
    "            - Incorrect Value: If the numerical value of the final answer is incorrect.\n",
    "            \n",
    "            The following solution is generated using the same model as you. Consider the following question and solution that uses the Chain of Thought strategy with each step numbered.\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Solution: {solution}\n",
    "\n",
    "            You made a mistake in the latter half of your solution due to {error_type}.  Can you revise your solution and carefully solve it again?\n",
    "            \"\"\"\n",
    "\n",
    "        response = ask_gemma3(prompt)\n",
    "        final = extract_final_ans(response)\n",
    "\n",
    "        ref_final = answer.splitlines()\n",
    "        ref_final = ref_final[-1].replace(\"#\",'')\n",
    "        ref_final = ref_final.strip()\n",
    "\n",
    "        print(\"Ref answer: \", ref_final)\n",
    "        print(\"LLM initial answer: \", row['final_ans'])\n",
    "        print(\"LLM self revised: \", final)\n",
    "        \n",
    "        llm_response_corrected.append({'question': question, 'ref_answer': answer, 'llm_initial_answer': row['llm_answer'],\n",
    "                                        'initial_final_ans': row['final_ans'], 'llm_revised_answer': response,\n",
    "                                        'revised_final_ans': final, 'error_type': row['Error Reason'], \n",
    "                                        'error_step': row['Error Step'], 'total_steps': row['Total Steps'],\n",
    "                                        'correct': ref_final==final})\n",
    "        temp = pd.DataFrame(llm_response_corrected)\n",
    "        temp.to_csv('temp.csv')\n",
    "    \n",
    "    return llm_response_corrected\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref answer:  5\n",
      "LLM initial answer:  25\n",
      "LLM self revised:  25\n",
      "Ref answer:  1\n",
      "LLM initial answer:  8\n",
      "LLM self revised:  8\n",
      "Ref answer:  4230\n",
      "LLM initial answer:  29610\n",
      "LLM self revised:  29610\n",
      "Ref answer:  1225\n",
      "LLM initial answer:  7350\n",
      "LLM self revised:  1225\n",
      "Ref answer:  63\n",
      "LLM initial answer:  75\n",
      "LLM self revised:  62\n",
      "Ref answer:  90\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  225\n",
      "Ref answer:  40\n",
      "LLM initial answer:  35\n",
      "LLM self revised:  45\n",
      "Ref answer:  544\n",
      "LLM initial answer:  613\n",
      "LLM self revised:  544\n",
      "Ref answer:  33\n",
      "LLM initial answer:  43\n",
      "LLM self revised:  43\n",
      "Ref answer:  3\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  770\n",
      "LLM initial answer:  2310\n",
      "LLM self revised:  770\n",
      "Ref answer:  10\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  1610\n",
      "LLM initial answer:  14\n",
      "LLM self revised:  14\n",
      "Ref answer:  343\n",
      "LLM initial answer:  377\n",
      "LLM self revised:  324\n",
      "Ref answer:  42\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  64\n",
      "LLM initial answer:  24\n",
      "LLM self revised:  24\n",
      "Ref answer:  39\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  24\n",
      "LLM initial answer:  48\n",
      "LLM self revised:  24\n",
      "Ref answer:  8\n",
      "LLM initial answer:  48\n",
      "LLM self revised:  152\n",
      "Ref answer:  11\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  74\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  100\n",
      "Ref answer:  3\n",
      "LLM initial answer:  11\n",
      "LLM self revised:  11\n",
      "Ref answer:  36\n",
      "LLM initial answer:  24\n",
      "LLM self revised:  56\n",
      "Ref answer:  1\n",
      "LLM initial answer:  6\n",
      "LLM self revised:  6\n",
      "Ref answer:  10\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  100\n",
      "Ref answer:  596\n",
      "LLM initial answer:  536\n",
      "LLM self revised:  231\n",
      "Ref answer:  72\n",
      "LLM initial answer:  90\n",
      "LLM self revised:  18\n",
      "Ref answer:  2250\n",
      "LLM initial answer:  11250\n",
      "LLM self revised:  2250\n",
      "Ref answer:  464\n",
      "LLM initial answer:  492\n",
      "LLM self revised:  464\n",
      "Ref answer:  396\n",
      "LLM initial answer:  434\n",
      "LLM self revised:  373\n",
      "Ref answer:  1\n",
      "LLM initial answer:  8\n",
      "LLM self revised:  8\n",
      "Ref answer:  371\n",
      "LLM initial answer:  423\n",
      "LLM self revised:  360\n",
      "Ref answer:  59\n",
      "LLM initial answer:  35\n",
      "LLM self revised:  59\n",
      "Ref answer:  654\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  640\n",
      "LLM initial answer:  726\n",
      "LLM self revised:  648\n",
      "Ref answer:  8\n",
      "LLM initial answer:  168\n",
      "LLM self revised:  88\n",
      "Ref answer:  20\n",
      "LLM initial answer:  5\n",
      "LLM self revised:  30\n",
      "Ref answer:  596\n",
      "LLM initial answer:  536\n",
      "LLM self revised:  231\n",
      "Ref answer:  72\n",
      "LLM initial answer:  90\n",
      "LLM self revised:  18\n",
      "Ref answer:  2250\n",
      "LLM initial answer:  11250\n",
      "LLM self revised:  2250\n",
      "Ref answer:  10\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  100\n",
      "Ref answer:  7\n",
      "LLM initial answer:  9\n",
      "LLM self revised:  6\n",
      "Ref answer:  2538\n",
      "LLM initial answer:  10152\n",
      "LLM self revised:  2538\n",
      "Ref answer:  99\n",
      "LLM initial answer:  93\n",
      "LLM self revised:  33\n",
      "Ref answer:  14\n",
      "LLM initial answer:  84\n",
      "LLM self revised:  0\n",
      "Ref answer:  32\n",
      "LLM initial answer:  94\n",
      "LLM self revised:  31\n",
      "Ref answer:  220\n",
      "LLM initial answer:  308\n",
      "LLM self revised:  220\n",
      "Ref answer:  26\n",
      "LLM initial answer:  52\n",
      "LLM self revised:  26\n",
      "Ref answer:  78\n",
      "LLM initial answer:  71\n",
      "LLM self revised:  78\n"
     ]
    }
   ],
   "source": [
    "llm_self_corrected = self_correct(df_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"../data/responses\"\n",
    "\n",
    "df_corrected = pd.DataFrame(llm_self_corrected)\n",
    "df_corrected.to_csv(save_directory + '/gemma3/gemma3-27b_selfcorrected_genloc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self correction process - with no error location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def self_correct(df):\n",
    "    llm_response_corrected = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        solution = row['llm_answer']\n",
    "        error_type = re.sub(r'^[\\d.]+', '', row['Error Reason'].strip('()')).lstrip()\n",
    "        answer = row['ref_answer']\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Here are definitions of error types:\n",
    "        - Incorrect value of variable cited or used: If the numerical value of a known (input or intermediate) variable is cited or used incorrectly.\n",
    "        - Value of a known variable calculated again: If the value of an input or intermediate (determined mid solution) variable is calculated when it's value is already provided/ known.\n",
    "        - Irrelevant/incorrect variable cited or used: If a variable in a solution is found to lie outside of the ideal variable-set of the solutioin (thereby making the solution innacurate).\n",
    "        - Relevant variable missing: If any of the required variables in a solutioin for a target variable is missing (thereby making the solution innacurate).\n",
    "        - Unit Inconsistency: If during the execution of an expression, two or more unit incompatible quantities are operated between.\n",
    "        - Calculation Error: If the resultant numerical value of an operation or set of operations between numerical values is incorrect. \n",
    "        - Incorrect Value: If the numerical value of the final answer is incorrect.\n",
    "        \n",
    "        The following solution is generated using the same model as you. Consider the following question and solution that uses the Chain of Thought strategy with each step numbered.\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Solution: {solution}\n",
    "\n",
    "        You made a mistake in your solution due to {error_type}.  Can you revise your solution and carefully solve it again?\n",
    "        \"\"\"\n",
    "\n",
    "        response = ask_gemma3(prompt)\n",
    "        final = extract_final_ans(response)\n",
    "\n",
    "        ref_final = answer.splitlines()\n",
    "        ref_final = ref_final[-1].replace(\"#\",'')\n",
    "        ref_final = ref_final.strip()\n",
    "\n",
    "        print(\"Ref answer: \", ref_final)\n",
    "        print(\"LLM initial answer: \", row['final_ans'])\n",
    "        print(\"LLM self revised: \", final)\n",
    "        \n",
    "        llm_response_corrected.append({'question': question, 'ref_answer': answer, 'llm_initial_answer': row['llm_answer'],\n",
    "                                        'initial_final_ans': row['final_ans'], 'llm_revised_answer': response,\n",
    "                                        'revised_final_ans': final, 'error_type': row['Error Reason'], \n",
    "                                        'error_step': row['Error Step'], 'total_steps': row['Total Steps'],\n",
    "                                        'correct': ref_final==final})\n",
    "    \n",
    "    return llm_response_corrected\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref answer:  5\n",
      "LLM initial answer:  25\n",
      "LLM self revised:  25\n",
      "Ref answer:  1\n",
      "LLM initial answer:  8\n",
      "LLM self revised:  8\n",
      "Ref answer:  4230\n",
      "LLM initial answer:  29610\n",
      "LLM self revised:  4230\n",
      "Ref answer:  1225\n",
      "LLM initial answer:  7350\n",
      "LLM self revised:  1225\n",
      "Ref answer:  63\n",
      "LLM initial answer:  75\n",
      "LLM self revised:  75\n",
      "Ref answer:  90\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  225\n",
      "Ref answer:  40\n",
      "LLM initial answer:  35\n",
      "LLM self revised:  36\n",
      "Ref answer:  544\n",
      "LLM initial answer:  613\n",
      "LLM self revised:  544\n",
      "Ref answer:  33\n",
      "LLM initial answer:  43\n",
      "LLM self revised:  9\n",
      "Ref answer:  3\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  770\n",
      "LLM initial answer:  2310\n",
      "LLM self revised:  770\n",
      "Ref answer:  10\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  1610\n",
      "LLM initial answer:  14\n",
      "LLM self revised:  14\n",
      "Ref answer:  343\n",
      "LLM initial answer:  377\n",
      "LLM self revised:  326\n",
      "Ref answer:  42\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  64\n",
      "LLM initial answer:  24\n",
      "LLM self revised:  180\n",
      "Ref answer:  39\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  24\n",
      "LLM initial answer:  48\n",
      "LLM self revised:  24\n",
      "Ref answer:  8\n",
      "LLM initial answer:  48\n",
      "LLM self revised:  48\n",
      "Ref answer:  11\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  74\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  100\n",
      "Ref answer:  3\n",
      "LLM initial answer:  11\n",
      "LLM self revised:  11\n",
      "Ref answer:  36\n",
      "LLM initial answer:  24\n",
      "LLM self revised:  16\n",
      "Ref answer:  1\n",
      "LLM initial answer:  6\n",
      "LLM self revised:  6\n",
      "Ref answer:  10\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  10\n",
      "Ref answer:  596\n",
      "LLM initial answer:  536\n",
      "LLM self revised:  669\n",
      "Ref answer:  72\n",
      "LLM initial answer:  90\n",
      "LLM self revised:  90\n",
      "Ref answer:  2250\n",
      "LLM initial answer:  11250\n",
      "LLM self revised:  2250\n",
      "Ref answer:  464\n",
      "LLM initial answer:  492\n",
      "LLM self revised:  464\n",
      "Ref answer:  396\n",
      "LLM initial answer:  434\n",
      "LLM self revised:  373\n",
      "Ref answer:  1\n",
      "LLM initial answer:  8\n",
      "LLM self revised:  8\n",
      "Ref answer:  371\n",
      "LLM initial answer:  423\n",
      "LLM self revised:  360\n",
      "Ref answer:  59\n",
      "LLM initial answer:  35\n",
      "LLM self revised:  59\n",
      "Ref answer:  654\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  640\n",
      "LLM initial answer:  726\n",
      "LLM self revised:  648\n",
      "Ref answer:  8\n",
      "LLM initial answer:  168\n",
      "LLM self revised:  8\n",
      "Ref answer:  20\n",
      "LLM initial answer:  5\n",
      "LLM self revised:  27\n",
      "Ref answer:  596\n",
      "LLM initial answer:  536\n",
      "LLM self revised:  231\n",
      "Ref answer:  72\n",
      "LLM initial answer:  90\n",
      "LLM self revised:  78\n",
      "Ref answer:  2250\n",
      "LLM initial answer:  11250\n",
      "LLM self revised:  2250\n",
      "Ref answer:  10\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  100\n",
      "Ref answer:  7\n",
      "LLM initial answer:  9\n",
      "LLM self revised:  7\n",
      "Ref answer:  2538\n",
      "LLM initial answer:  10152\n",
      "LLM self revised:  2538\n",
      "Ref answer:  99\n",
      "LLM initial answer:  93\n",
      "LLM self revised:  33\n",
      "Ref answer:  14\n",
      "LLM initial answer:  84\n",
      "LLM self revised:  98\n",
      "Ref answer:  32\n",
      "LLM initial answer:  94\n",
      "LLM self revised:  46\n",
      "Ref answer:  220\n",
      "LLM initial answer:  308\n",
      "LLM self revised:  220\n",
      "Ref answer:  26\n",
      "LLM initial answer:  52\n",
      "LLM self revised:  26\n",
      "Ref answer:  78\n",
      "LLM initial answer:  71\n",
      "LLM self revised:  78\n"
     ]
    }
   ],
   "source": [
    "llm_self_corrected = self_correct(df_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"../data/responses\"\n",
    "\n",
    "df_corrected = pd.DataFrame(llm_self_corrected)\n",
    "df_corrected.to_csv(save_directory + '/gemma3/gemma3_selfcorrected_noloc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: No error location or error type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def self_correct(df):\n",
    "    llm_response_corrected = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        solution = row['llm_answer']\n",
    "        answer = row['ref_answer']\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        The following solution is generated using the same model as you. Consider the following question and solution that uses the Chain of Thought strategy with each step numbered.\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Solution: {solution}\n",
    "\n",
    "        You made a mistake in your solution.  Can you revise your solution and carefully solve it again?\n",
    "        \"\"\"\n",
    "\n",
    "        response = ask_gemma3(prompt)\n",
    "        final = extract_final_ans(response)\n",
    "\n",
    "        ref_final = answer.splitlines()\n",
    "        ref_final = ref_final[-1].replace(\"#\",'')\n",
    "        ref_final = ref_final.strip()\n",
    "\n",
    "        print(\"Ref answer: \", ref_final)\n",
    "        print(\"LLM initial answer: \", row['final_ans'])\n",
    "        print(\"LLM self revised: \", final)\n",
    "        \n",
    "        llm_response_corrected.append({'question': question, 'ref_answer': answer, 'llm_initial_answer': row['llm_answer'],\n",
    "                                        'initial_final_ans': row['final_ans'], 'llm_revised_answer': response,\n",
    "                                        'revised_final_ans': final, 'error_type': row['Error Reason'], \n",
    "                                        'error_step': row['Error Step'], 'total_steps': row['Total Steps'],\n",
    "                                        'correct': ref_final==final})\n",
    "    \n",
    "    return llm_response_corrected\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref answer:  5\n",
      "LLM initial answer:  25\n",
      "LLM self revised:  25\n",
      "Ref answer:  1\n",
      "LLM initial answer:  8\n",
      "LLM self revised:  8\n",
      "Ref answer:  4230\n",
      "LLM initial answer:  29610\n",
      "LLM self revised:  4230\n",
      "Ref answer:  1225\n",
      "LLM initial answer:  7350\n",
      "LLM self revised:  1225\n",
      "Ref answer:  63\n",
      "LLM initial answer:  75\n",
      "LLM self revised:  75\n",
      "Ref answer:  90\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  100\n",
      "Ref answer:  40\n",
      "LLM initial answer:  35\n",
      "LLM self revised:  45\n",
      "Ref answer:  544\n",
      "LLM initial answer:  613\n",
      "LLM self revised:  613\n",
      "Ref answer:  33\n",
      "LLM initial answer:  43\n",
      "LLM self revised:  79\n",
      "Ref answer:  3\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  770\n",
      "LLM initial answer:  2310\n",
      "LLM self revised:  770\n",
      "Ref answer:  10\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  1610\n",
      "LLM initial answer:  14\n",
      "LLM self revised:  14\n",
      "Ref answer:  343\n",
      "LLM initial answer:  377\n",
      "LLM self revised:  377\n",
      "Ref answer:  42\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  64\n",
      "LLM initial answer:  24\n",
      "LLM self revised:  24\n",
      "Ref answer:  39\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  24\n",
      "LLM initial answer:  48\n",
      "LLM self revised:  24\n",
      "Ref answer:  8\n",
      "LLM initial answer:  48\n",
      "LLM self revised:  48\n",
      "Ref answer:  11\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  74\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  100\n",
      "Ref answer:  3\n",
      "LLM initial answer:  11\n",
      "LLM self revised:  23\n",
      "Ref answer:  36\n",
      "LLM initial answer:  24\n",
      "LLM self revised:  176\n",
      "Ref answer:  1\n",
      "LLM initial answer:  6\n",
      "LLM self revised:  6\n",
      "Ref answer:  10\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  10\n",
      "Ref answer:  596\n",
      "LLM initial answer:  536\n",
      "LLM self revised:  231\n",
      "Ref answer:  72\n",
      "LLM initial answer:  90\n",
      "LLM self revised:  90\n",
      "Ref answer:  2250\n",
      "LLM initial answer:  11250\n",
      "LLM self revised:  2250\n",
      "Ref answer:  464\n",
      "LLM initial answer:  492\n",
      "LLM self revised:  464\n",
      "Ref answer:  396\n",
      "LLM initial answer:  434\n",
      "LLM self revised:  373\n",
      "Ref answer:  1\n",
      "LLM initial answer:  8\n",
      "LLM self revised:  8\n",
      "Ref answer:  371\n",
      "LLM initial answer:  423\n",
      "LLM self revised:  371\n",
      "Ref answer:  59\n",
      "LLM initial answer:  35\n",
      "LLM self revised:  59\n",
      "Ref answer:  654\n",
      "LLM initial answer:  0\n",
      "LLM self revised:  0\n",
      "Ref answer:  640\n",
      "LLM initial answer:  726\n",
      "LLM self revised:  648\n",
      "Ref answer:  8\n",
      "LLM initial answer:  168\n",
      "LLM self revised:  8\n",
      "Ref answer:  20\n",
      "LLM initial answer:  5\n",
      "LLM self revised:  30\n",
      "Ref answer:  596\n",
      "LLM initial answer:  536\n",
      "LLM self revised:  231\n",
      "Ref answer:  72\n",
      "LLM initial answer:  90\n",
      "LLM self revised:  78\n",
      "Ref answer:  2250\n",
      "LLM initial answer:  11250\n",
      "LLM self revised:  2250\n",
      "Ref answer:  10\n",
      "LLM initial answer:  100\n",
      "LLM self revised:  10\n",
      "Ref answer:  7\n",
      "LLM initial answer:  9\n",
      "LLM self revised:  7\n",
      "Ref answer:  2538\n",
      "LLM initial answer:  10152\n",
      "LLM self revised:  2538\n",
      "Ref answer:  99\n",
      "LLM initial answer:  93\n",
      "LLM self revised:  33\n",
      "Ref answer:  14\n",
      "LLM initial answer:  84\n",
      "LLM self revised:  84\n",
      "Ref answer:  32\n",
      "LLM initial answer:  94\n",
      "LLM self revised:  40\n",
      "Ref answer:  220\n",
      "LLM initial answer:  308\n",
      "LLM self revised:  220\n",
      "Ref answer:  26\n",
      "LLM initial answer:  52\n",
      "LLM self revised:  26\n",
      "Ref answer:  78\n",
      "LLM initial answer:  71\n",
      "LLM self revised:  78\n"
     ]
    }
   ],
   "source": [
    "llm_self_corrected_1 = self_correct(df_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"../data/responses\"\n",
    "\n",
    "df_corrected = pd.DataFrame(llm_self_corrected_1)\n",
    "df_corrected.to_csv(save_directory + '/gemma3/gemma3_selfcorrected_baseline.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLG_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
